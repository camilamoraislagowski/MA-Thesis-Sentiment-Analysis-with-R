‘AI will never threaten humans, says top Meta scientist’ | Artificial intelligence is still dumber than cats, says pioneer Yann LeCun, so worries over existential risks are ‘premature’ | Financial Times 
https://www.ft.com/content/30fa44a1-7623-499f-93b0-81e26e22f2a6




John Thornhill 
OCTOBER 19 2023




Premature regulation of artificial intelligence will only serve to reinforce the dominance of the big technology companies and stifle competition, Yann LeCun, Meta’s chief AI scientist, has said.


“Regulating research and development in AI is incredibly counterproductive,” LeCun, one of the world’s leading AI researchers, told the Financial Times ahead of next month’s Bletchley Park conference on AI safety hosted by the British government. “They want regulatory capture under the guise of AI safety.”


Demands to police AI stemmed from the “superiority complex” of some of the leading tech companies that argued that only they could be trusted to develop AI safely, LeCun said. “I think that’s incredibly arrogant. And I think the exact opposite,” he said in an interview for the FT’s forthcoming Tech Tonic podcast series.


Regulating leading-edge AI models today would be like regulating the jet airline industry in 1925 when such aeroplanes had not even been invented, he said. “The debate on existential risk is very premature until we have a design for a system that can even rival a cat in terms of learning capabilities, which we don’t have at the moment,” he said.


Meta, which has launched its own LLaMA generative AI model, has broken ranks with other big tech companies, such as Google and Microsoft-backed OpenAI, in championing more accessible open-source AI systems. OpenAI’s latest model GPT-4 is a so-called black box in which the data and code used to build the model are not available to third parties.


LeCun argued that open-source models stimulated competition and enabled a greater diversity of people to build and use AI systems.


But critics fear that placing powerful generative AI models in the hands of potentially bad actors magnifies the risks of industrial-scale disinformation, cyber warfare and bioterrorism.


Similar arguments about the necessity of controlling fast-evolving technology, LeCun said, had been made at the start of the internet but that technology had only flourished because it had remained an open, decentralised platform.


“The same thing will happen with AI,” he said.


LeCun confirmed he was participating in the two-day summit that kicks off on November 1 at the second world war codebreakers’ hub in the English countryside not far from London.


LeCun is one of the world’s leading researchers in deep neural networks, which have underpinned the latest advances in generative AI. In 2018 he jointly won the Turing Award for computer science with Geoffrey Hinton and Yoshua Bengio.


But since the emergence of powerful generative AI models, such as ChatGPT, Hinton and Bengio have expressed alarm about the dangers posed by next-generation AI models. Both have called for a slowdown in the development of leading-edge models and have warned about the possible existential risks of AI.


However, LeCun dismissed the idea that AI might kill humanity, by design or default, as “preposterous”. He said people had been conditioned by science fiction and the Terminator scenario to believe that intelligent machines would take control if they became smarter than humans.


“Intelligence has nothing to do with a desire to dominate. It’s not even true for humans,” he said. “If it were true that the smartest humans wanted to dominate others, then Albert Einstein and other scientists would have been both rich and powerful, and they were neither.”


LeCun said the current generation of AI models were not nearly as capable as some researchers claimed. “They just do not understand how the world works. They’re not capable of planning. They’re not capable of real reasoning,” he said. “We do not have completely autonomous, self-driving cars that can train themselves to drive in about 20 hours of practice, something a 17-year-old can do,” he said.


The Meta researcher said OpenAI and Google DeepMind had been “consistently over-optimistic” about the complexity of the problem and that several “conceptual breakthroughs” were still needed before AI systems approached human-level intelligence. But even then, they could be controlled by encoding “moral character” into these systems in the same way as people enact laws to govern human behaviour.


LeCun acknowledged that machines would one day be more intelligent than humans in most domains but they would stimulate a second Renaissance in learning. Powerful AI systems would help humanity tackle big challenges, such as combating climate change and curing diseases, he said.


“There’s no question that we’ll have machines assisting us that are smarter than us. And the question is: is that scary or is that exciting? I think it’s exciting because those machines will be doing our bidding,” he said. “They will be under our control.”


Meta has already incorporated AI into many of its services, including safety, security and content moderation, and it would be more deeply embedded in the company’s augmented and virtual reality applications in future, LeCun said.


Eventually, everyone could access AI assistants that would help manage our daily lives. “Everyone’s interaction with the digital world will be mediated by AI systems. In other words, we’re not going to use search engines anymore,” he said.